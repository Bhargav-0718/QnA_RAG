Agent Workflow Report

The agent is designed as a Retrieval-Augmented Generation (RAG) system for answering questions about actors based on their wiki PDFs. The workflow is structured into four main steps: plan → retrieve → answer → reflect. First, the plan node interprets the user query and decides whether external context is required. If context is needed, the retrieve node fetches the top-k relevant chunks from a Chroma vectorstore, which stores embeddings generated from the PDF texts using OpenAI embeddings. The answer node then uses an LLM (e.g., GPT-4o-mini) to generate a concise answer based on the retrieved context. Finally, the reflect node evaluates the completeness and relevance of the answer and provides metadata such as whether source chunks were cited and whether the answer is sufficiently detailed.

During implementation, one of the challenges encountered was integrating the LLM-based evaluation. Ideally, the agent would generate an evaluation score by comparing its answer against reference answers. However, since no reference answers were available for the actor PDF dataset, this step could not be fully implemented. Consequently, the evaluation parameter has been skipped in the current version, and the system focuses on providing answers and reflections without automated scoring. This limitation highlights the importance of curated reference datasets for RAG evaluation, especially when relying on LLMs as judges.